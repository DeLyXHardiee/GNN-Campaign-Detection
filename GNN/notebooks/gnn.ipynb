{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9661af56",
   "metadata": {},
   "source": [
    "### 1. Import IMDB-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cec87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a notebook cell (run once per kernel)\n",
    "import os, sys\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")  # if the notebook is in notebooks/, go up one level\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from src.load_graph_data import load_hetero_pt, load_imdb\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "\n",
    "data = load_imdb()\n",
    "\n",
    "data = ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d646cc",
   "metadata": {},
   "source": [
    "### 1.1 Quick sanity check of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c04d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- Print a quick summary ----------\n",
    "print(\"=== Data loaded ===\")\n",
    "print(\"Metadata (node_types, edge_types):\")\n",
    "print(data.metadata())  # (['movie','director','actor'], [('movie','to','director'), ...])\n",
    "\n",
    "# Basic counts per node type\n",
    "print(\"\\nNode counts:\")\n",
    "for ntype in data.node_types:\n",
    "    print(f\"  {ntype:>12}: {data[ntype].num_nodes}\")\n",
    "\n",
    "# Basic counts per edge type\n",
    "print(\"\\nEdge counts:\")\n",
    "for et in data.edge_types:\n",
    "    E = data[et].edge_index.size(1)\n",
    "    print(f\"  {et}: {E}\")\n",
    "\n",
    "# Peek at feature availability\n",
    "print(\"\\nFeature tensors present?\")\n",
    "for ntype in data.node_types:\n",
    "    has_x = 'x' in data[ntype]\n",
    "    shape = tuple(data[ntype].x.shape) if has_x else None\n",
    "    print(f\"  {ntype:>12}: x present? {has_x}, shape={shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                      else \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "                      else \"cpu\")\n",
    "\n",
    "# Pick any integer seed\n",
    "TORCH_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train again with stronger negatives; keep your fanout as before\n",
    "import os, sys\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")  # if notebook is in notebooks/\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from src.train import run_training\n",
    "from src.eval_link import collect_scores, topk_eval_with_splits\n",
    "\n",
    "\n",
    "\n",
    "model, predictor, loaders, splits = run_training(\n",
    "    DEVICE,\n",
    "    TORCH_SEED,\n",
    "    data,\n",
    "    primary_ntype='movie',\n",
    "    hidden=128, out_dim=128, layers=2, dropout=0.1,\n",
    "    neg_ratio=3.0,                # <— more negatives per positive\n",
    "    batch_size=1024, fanout=[15, 10],\n",
    "    val_ratio=0.1, test_ratio=0.1, epochs=10, lr=1e-3, wd=1e-4,\n",
    "    score_head='dot'              # or 'mlp' to try the MLP scorer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f511e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07714a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUROC/AP (as before)\n",
    "scores = collect_scores(DEVICE, model, predictor, loaders['test'])\n",
    "print(\"TEST AUROC/AP per relation:\")\n",
    "for et, s in scores.items():\n",
    "    print(et, s)\n",
    "\n",
    "# Recall@K with cosine and larger K (easier for many-to-many like movie→actor)\n",
    "res_k50_cos = topk_eval_with_splits(model, splits, splits['sup_ets'], K=50, use_dot=False)\n",
    "print(\"\\nRecall@50 (cosine) per relation:\")\n",
    "for et, m in res_k50_cos.items():\n",
    "    print(et, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram (held-out vs random) — pick one relation\n",
    "import matplotlib.pyplot as plt, torch\n",
    "h_train = embed_with_graph(model, splits['train_graph'])\n",
    "et = ('movie','to','director')\n",
    "S = h_train[et[0]]; D = h_train[et[2]]\n",
    "pos = splits['test_pos'][et]\n",
    "pos_scores = (S[pos[0]] * D[pos[1]]).sum(dim=1).cpu().numpy()\n",
    "g = torch.Generator().manual_seed(0)\n",
    "neg_src = torch.randint(0, S.size(0), (len(pos_scores),), generator=g)\n",
    "neg_dst = torch.randint(0, D.size(0), (len(pos_scores),), generator=g)\n",
    "neg_scores = (S[neg_src] * D[neg_dst]).sum(dim=1).cpu().numpy()\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(pos_scores, bins=50, alpha=0.6, label='Held-out positives')\n",
    "plt.hist(neg_scores, bins=50, alpha=0.6, label='Random negatives')\n",
    "plt.title(f\"Score distributions for {et}\"); plt.legend(); plt.show()\n",
    "\n",
    "# Qualitative Top-K (choose a source that *has* held-out positives)\n",
    "sources_with_pos = splits['test_pos'][et][0].unique()\n",
    "src_id = int(sources_with_pos[1])  # pick first valid movie\n",
    "top_ids, top_scores = topk_for_source(h_train, et, src_id, K=20, cosine=True)\n",
    "true_dests = set(splits['test_pos'][et][1][splits['test_pos'][et][0]==src_id].cpu().tolist())\n",
    "print(\"movie_id:\", src_id)\n",
    "print(\"Top-20 actor ids:\", top_ids)\n",
    "print(\"Is held-out true?:\", [i in true_dests for i in top_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Light suggestions: recall curves, cosine vs dot, qualitative Top-K, optional fine-tune ===\n",
    "import torch, matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Recall@K curves (cosine) ---\n",
    "K_list = [10, 20, 50, 100]\n",
    "h_train = embed_with_graph(model, splits['train_graph'])  # leakage-safe embeddings\n",
    "ets = splits['sup_ets']\n",
    "\n",
    "recall_curves = {et: [] for et in ets}\n",
    "for et in ets:\n",
    "    for K in K_list:\n",
    "        res = recall_at_k_mrr(h_train, et, splits['test_pos'][et], K=K, use_dot=False)\n",
    "        recall_curves[et].append(res['recall@K'])\n",
    "\n",
    "# Plot curves\n",
    "plt.figure(figsize=(7,5))\n",
    "for et, vals in recall_curves.items():\n",
    "    label = f\"{et[0]}→{et[2]}\"\n",
    "    plt.plot(K_list, vals, marker='o', label=label)\n",
    "plt.xlabel(\"K\"); plt.ylabel(\"Recall@K\"); plt.title(\"Recall@K (cosine) per relation\"); plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- 2) Dot vs Cosine table at K=50 ---\n",
    "def recall_at_k_for_metric(h, et, K, use_dot):\n",
    "    return recall_at_k_mrr(h, et, splits['test_pos'][et], K=K, use_dot=use_dot)['recall@K']\n",
    "\n",
    "print(\"\\nRecall@50: dot vs cosine\")\n",
    "for et in ets:\n",
    "    r_dot = recall_at_k_for_metric(h_train, et, K=50, use_dot=True)\n",
    "    r_cos = recall_at_k_for_metric(h_train, et, K=50, use_dot=False)\n",
    "    print(f\"{et}: dot={r_dot:.3f} | cosine={r_cos:.3f}\")\n",
    "\n",
    "# --- 3) Qualitative Top-K (pick a source that has held-out positives) ---\n",
    "et_demo = ('movie','to','actor')  # change if you want a different relation\n",
    "srcs_with_pos = splits['test_pos'][et_demo][0].unique()\n",
    "if len(srcs_with_pos) > 0:\n",
    "    src_id = int(srcs_with_pos[0])  # first valid source\n",
    "    top_ids, top_scores = topk_for_source(h_train, et_demo, src_id, K=20, cosine=True)\n",
    "    true_dests = set(splits['test_pos'][et_demo][1][splits['test_pos'][et_demo][0]==src_id].cpu().tolist())\n",
    "    print(f\"\\nQualitative Top-20 for source {et_demo[0]} id={src_id} (cosine):\")\n",
    "    print(\"Top-20 dest ids:\", top_ids)\n",
    "    print(\"Is held-out true?:\", [i in true_dests for i in top_ids])\n",
    "else:\n",
    "    print(f\"No sources with held-out positives found for {et_demo}\")\n",
    "\n",
    "# --- 4) (Optional) Fine-tune a bit more, then re-run the same checks ---\n",
    "# Uncomment to do 2 more epochs with same loaders/optimizer settings:\n",
    "# opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# for e in range(2):\n",
    "#     tr_loss, tr_acc = train_epoch(model, predictor, opt, loaders['train'])\n",
    "#     va_loss, va_acc = eval_epoch(model, predictor, loaders['val'])\n",
    "#     print(f\"[Finetune {e+1}/2] train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f}\")\n",
    "# # Recompute embeddings & recall curves if you fine-tuned:\n",
    "# # h_train = embed_with_graph(model, splits['train_graph'])\n",
    "# # (then rerun the recall curve and Top-K blocks above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72335494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export embeddings for the primary node type (e.g., 'movie') ---\n",
    "\n",
    "\n",
    "# Use the splits you already have (leakage-safe train_graph)\n",
    "z = export_embeddings(model, splits['train_graph'], primary_ntype='movie', layers=2, batch_size=4096)\n",
    "\n",
    "# L2-normalize (cosine-friendly)\n",
    "z = torch.nn.functional.normalize(z, p=2, dim=1)  # [N, d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce026820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_graph_cosine(z, k=30, chunk=8192):\n",
    "    \"\"\"\n",
    "    Returns (indices, sims), where:\n",
    "      indices: [N, k] int64 neighbor IDs (excluding self)\n",
    "      sims:    [N, k] float32 cosine sims corresponding to indices\n",
    "    \"\"\"\n",
    "    z = torch.nn.functional.normalize(z, p=2, dim=1)\n",
    "    N, d = z.shape\n",
    "    all_idx = []\n",
    "    all_sim = []\n",
    "    for start in range(0, N, chunk):\n",
    "        end = min(start+chunk, N)\n",
    "        block = z[start:end]                               # [B, d]\n",
    "        sims = block @ z.T                                 # [B, N]\n",
    "        sims[:, start:end] = -1.0                          # exclude self-range; will be masked out by topk anyway\n",
    "        vals, idx = torch.topk(sims, k=k, dim=1)           # [B, k]\n",
    "        all_idx.append(idx.cpu())\n",
    "        all_sim.append(vals.cpu())\n",
    "    return torch.vstack(all_idx), torch.vstack(all_sim)\n",
    "\n",
    "knn_idx, knn_sim = knn_graph_cosine(z, k=30)  # typical k=15~50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c48746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hdbscan (once)\n",
    "import numpy as np\n",
    "try:\n",
    "    import hdbscan\n",
    "except ImportError:\n",
    "    raise RuntimeError(\"Please `pip install hdbscan` to use HDBSCAN.\")\n",
    "\n",
    "def cluster_hdbscan(z, min_cluster_size=15, min_samples=None, metric='euclidean'):\n",
    "    # HDBSCAN works in distance space; use euclidean on normalized z (cosine≈euclid on unit vectors)\n",
    "    Z = z.numpy() if isinstance(z, torch.Tensor) else z\n",
    "    clf = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric=metric)\n",
    "    labels = clf.fit_predict(Z)    # -1 are noise points\n",
    "    probs  = clf.probabilities_\n",
    "    return labels, probs, clf\n",
    "\n",
    "labels_hdb, probs_hdb, hdb = cluster_hdbscan(z, min_cluster_size=20, min_samples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc352ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install igraph leidenalg\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "def leiden_from_knn(knn_idx, knn_sim=None, resolution=1.0, weighted=True):\n",
    "    \"\"\"\n",
    "    Build an undirected graph from kNN edges and run Leiden.\n",
    "    \"\"\"\n",
    "    N = knn_idx.shape[0]\n",
    "    # Build edge list (i < j to avoid duplicates)\n",
    "    src = np.repeat(np.arange(N), knn_idx.shape[1])\n",
    "    dst = knn_idx.reshape(-1)\n",
    "    edge_pairs = np.stack([src, dst], axis=1)\n",
    "    # Make undirected unique edges\n",
    "    edge_pairs = np.sort(edge_pairs, axis=1)\n",
    "    edge_pairs = np.unique(edge_pairs, axis=0)\n",
    "    g = ig.Graph(n=N, edges=edge_pairs.tolist(), directed=False)\n",
    "\n",
    "    weights = None\n",
    "    if weighted and knn_sim is not None:\n",
    "        # Map weights per edge; we take max of (i->j, j->i) if duplicates happened before unique\n",
    "        sim_map = {}\n",
    "        for i in range(N):\n",
    "            for k, j in enumerate(knn_idx[i]):\n",
    "                a, b = (i, int(j))\n",
    "                key = (min(a,b), max(a,b))\n",
    "                w = float(knn_sim[i, k])\n",
    "                sim_map[key] = max(sim_map.get(key, -1e9), w)\n",
    "        weights = [sim_map[(a,b)] for a,b in edge_pairs]\n",
    "\n",
    "    part = la.find_partition(g, la.RBConfigurationVertexPartition, weights=weights, resolution_parameter=resolution)\n",
    "    return np.array(part.membership), g, part\n",
    "\n",
    "labels_lei, g_lei, part_lei = leiden_from_knn(knn_idx.numpy(), knn_sim.numpy(), resolution=1.0, weighted=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910bcb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def intrinsic_metrics(z, labels, max_points=20000):\n",
    "    # ignore noise label -1 for silhouette if present\n",
    "    z_np = z.numpy() if isinstance(z, torch.Tensor) else z\n",
    "    idx = np.arange(len(labels))\n",
    "    mask = labels != -1\n",
    "    use = idx[mask]\n",
    "    if len(use) < 2 or len(np.unique(labels[mask])) < 2:\n",
    "        return {'silhouette': np.nan, 'calinski_harabasz': np.nan, 'davies_bouldin': np.nan}\n",
    "    # downsample for speed\n",
    "    if len(use) > max_points:\n",
    "        use = np.random.RandomState(0).choice(use, size=max_points, replace=False)\n",
    "    s = silhouette_score(z_np[use], labels[use], metric='euclidean')\n",
    "    ch = calinski_harabasz_score(z_np[use], labels[use])\n",
    "    db = davies_bouldin_score(z_np[use], labels[use])\n",
    "    return {'silhouette': float(s), 'calinski_harabasz': float(ch), 'davies_bouldin': float(db)}\n",
    "\n",
    "m_in_hdb = intrinsic_metrics(z, labels_hdb)\n",
    "m_in_lei = intrinsic_metrics(z, labels_lei)\n",
    "print(\"HDBSCAN intrinsic:\", m_in_hdb)\n",
    "print(\"Leiden   intrinsic:\", m_in_lei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_purity_movie_director(labels, data, primary_ntype='movie'):\n",
    "    \"\"\"\n",
    "    For each movie, get its directors. For each cluster, take the majority director and compute purity.\n",
    "    Returns micro- and macro-averaged purity (exclude noise cluster -1).\n",
    "    \"\"\"\n",
    "    ei = data[('movie','to','director')].edge_index  # [2, E]\n",
    "    m, d = ei[0].cpu().numpy(), ei[1].cpu().numpy()\n",
    "    N = data[primary_ntype].num_nodes\n",
    "    labels = np.asarray(labels)\n",
    "    # build directors-per-movie lists\n",
    "    from collections import defaultdict, Counter\n",
    "    dirs_by_movie = defaultdict(list)\n",
    "    for mi, di in zip(m, d):\n",
    "        dirs_by_movie[int(mi)].append(int(di))\n",
    "    # per-cluster majority director\n",
    "    cluster_to_movies = {}\n",
    "    for mi in range(N):\n",
    "        c = int(labels[mi])\n",
    "        if c == -1:   # skip noise\n",
    "            continue\n",
    "        cluster_to_movies.setdefault(c, []).append(mi)\n",
    "    if not cluster_to_movies:\n",
    "        return {'micro_purity': np.nan, 'macro_purity': np.nan, 'n_clusters': 0}\n",
    "\n",
    "    purities = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for c, movies in cluster_to_movies.items():\n",
    "        # count directors across all movies in cluster\n",
    "        cnt = Counter()\n",
    "        for mi in movies:\n",
    "            cnt.update(dirs_by_movie.get(mi, []))\n",
    "        if len(cnt) == 0:\n",
    "            purities.append(0.0)\n",
    "            continue\n",
    "        maj_dir, maj_count = cnt.most_common(1)[0]\n",
    "        # total assignments = number of (movie, director) pairs in cluster\n",
    "        cluster_total = sum(cnt.values())\n",
    "        purities.append(maj_count / cluster_total)\n",
    "        total += cluster_total\n",
    "        correct += maj_count\n",
    "    micro = correct / total if total > 0 else np.nan\n",
    "    macro = float(np.mean(purities)) if purities else np.nan\n",
    "    return {'micro_purity': micro, 'macro_purity': macro, 'n_clusters': len(cluster_to_movies)}\n",
    "\n",
    "pur_hdb = cluster_purity_movie_director(labels_hdb, data, 'movie')\n",
    "pur_lei = cluster_purity_movie_director(labels_lei, data, 'movie')\n",
    "print(\"HDBSCAN movie→director purity:\", pur_hdb)\n",
    "print(\"Leiden   movie→director purity:\", pur_lei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn (usually present)\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "def stability_ari_nmi(z, cluster_fn, n_runs=3):\n",
    "    parts = []\n",
    "    for s in range(n_runs):\n",
    "        np.random.seed(42 + s)\n",
    "        torch.manual_seed(42 + s)\n",
    "        labels, *_ = cluster_fn()\n",
    "        parts.append(np.asarray(labels))\n",
    "    # pairwise ARI/NMI\n",
    "    aris, nmis = [], []\n",
    "    for i in range(n_runs):\n",
    "        for j in range(i+1, n_runs):\n",
    "            aris.append(adjusted_rand_score(parts[i], parts[j]))\n",
    "            nmis.append(normalized_mutual_info_score(parts[i], parts[j]))\n",
    "    return {'ARI_mean': float(np.mean(aris)), 'NMI_mean': float(np.mean(nmis))}\n",
    "\n",
    "# Example: HDBSCAN stability over different min_samples\n",
    "stab_hdb = stability_ari_nmi(\n",
    "    z,\n",
    "    cluster_fn=lambda: cluster_hdbscan(z, min_cluster_size=20, min_samples=np.random.randint(5, 15)),\n",
    "    n_runs=3\n",
    ")\n",
    "print(\"HDBSCAN stability:\", stab_hdb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap-learn\n",
    "import umap\n",
    "def umap_embed(z, n_neighbors=30, min_dist=0.1, random_state=42):\n",
    "    Z = z.numpy() if isinstance(z, torch.Tensor) else z\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, random_state=random_state)\n",
    "    return reducer.fit_transform(Z)  # [N, 2]\n",
    "\n",
    "um = umap_embed(z, n_neighbors=30, min_dist=0.05)\n",
    "# Plot with your favorite tool; color by labels_hdb or labels_lei\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(um[:,0], um[:,1], c=labels_hdb, s=5, cmap='tab20', alpha=0.9)\n",
    "plt.title(\"UMAP of movie embeddings — HDBSCAN clusters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7eeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Export embeddings (leakage-safe) and normalize\n",
    "z = export_embeddings(model, splits['train_graph'], primary_ntype='movie', layers=2, batch_size=4096)\n",
    "z = torch.nn.functional.normalize(z, p=2, dim=1)\n",
    "\n",
    "# 2) k-NN graph\n",
    "knn_idx, knn_sim = knn_graph_cosine(z, k=30)\n",
    "\n",
    "# 3) Clustering\n",
    "labels_hdb, probs_hdb, _ = cluster_hdbscan(z, min_cluster_size=20, min_samples=10)\n",
    "labels_lei, g_lei, part_lei = leiden_from_knn(knn_idx.numpy(), knn_sim.numpy(), resolution=1.0, weighted=True)\n",
    "\n",
    "# 4) Metrics\n",
    "print(\"Intrinsic (HDBSCAN):\", intrinsic_metrics(z, labels_hdb))\n",
    "print(\"Intrinsic (Leiden):  \", intrinsic_metrics(z, labels_lei))\n",
    "print(\"Purity (movie→director, HDBSCAN):\", cluster_purity_movie_director(labels_hdb, data))\n",
    "print(\"Purity (movie→director, Leiden): \", cluster_purity_movie_director(labels_lei, data))\n",
    "\n",
    "# 5) Stability (optional)\n",
    "stab_hdb = stability_ari_nmi(z, lambda: cluster_hdbscan(z, 20, np.random.randint(5, 15)), n_runs=3)\n",
    "print(\"HDBSCAN stability:\", stab_hdb)\n",
    "\n",
    "# 6) UMAP (optional)\n",
    "um = umap_embed(z, n_neighbors=30, min_dist=0.05)\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(um[:,0], um[:,1], c=labels_hdb, s=5, cmap='tab20', alpha=0.9)\n",
    "plt.title(\"UMAP of movie embeddings — HDBSCAN clusters\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn (MPS)",
   "language": "python",
   "name": "gnn-mps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
