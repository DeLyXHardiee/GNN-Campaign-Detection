{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc21599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export embeddings for the primary node type (e.g., 'movie') ---\n",
    "\n",
    "# Use the splits you already have (leakage-safe train_graph)\n",
    "z = export_embeddings(model, splits['train_graph'], primary_ntype='movie', layers=2, batch_size=4096)\n",
    "\n",
    "# L2-normalize (cosine-friendly)\n",
    "z = torch.nn.functional.normalize(z, p=2, dim=1)  # [N, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fafe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_graph_cosine(z, k=30, chunk=8192):\n",
    "    \"\"\"\n",
    "    Returns (indices, sims), where:\n",
    "      indices: [N, k] int64 neighbor IDs (excluding self)\n",
    "      sims:    [N, k] float32 cosine sims corresponding to indices\n",
    "    \"\"\"\n",
    "    z = torch.nn.functional.normalize(z, p=2, dim=1)\n",
    "    N, d = z.shape\n",
    "    all_idx = []\n",
    "    all_sim = []\n",
    "    for start in range(0, N, chunk):\n",
    "        end = min(start+chunk, N)\n",
    "        block = z[start:end]                               # [B, d]\n",
    "        sims = block @ z.T                                 # [B, N]\n",
    "        sims[:, start:end] = -1.0                          # exclude self-range; will be masked out by topk anyway\n",
    "        vals, idx = torch.topk(sims, k=k, dim=1)           # [B, k]\n",
    "        all_idx.append(idx.cpu())\n",
    "        all_sim.append(vals.cpu())\n",
    "    return torch.vstack(all_idx), torch.vstack(all_sim)\n",
    "\n",
    "knn_idx, knn_sim = knn_graph_cosine(z, k=30)  # typical k=15~50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd22331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hdbscan (once)\n",
    "import numpy as np\n",
    "try:\n",
    "    import hdbscan\n",
    "except ImportError:\n",
    "    raise RuntimeError(\"Please `pip install hdbscan` to use HDBSCAN.\")\n",
    "\n",
    "def cluster_hdbscan(z, min_cluster_size=15, min_samples=None, metric='euclidean'):\n",
    "    # HDBSCAN works in distance space; use euclidean on normalized z (cosine≈euclid on unit vectors)\n",
    "    Z = z.numpy() if isinstance(z, torch.Tensor) else z\n",
    "    clf = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric=metric)\n",
    "    labels = clf.fit_predict(Z)    # -1 are noise points\n",
    "    probs  = clf.probabilities_\n",
    "    return labels, probs, clf\n",
    "\n",
    "labels_hdb, probs_hdb, hdb = cluster_hdbscan(z, min_cluster_size=20, min_samples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9470e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install igraph leidenalg\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "def leiden_from_knn(knn_idx, knn_sim=None, resolution=1.0, weighted=True):\n",
    "    \"\"\"\n",
    "    Build an undirected graph from kNN edges and run Leiden.\n",
    "    \"\"\"\n",
    "    N = knn_idx.shape[0]\n",
    "    # Build edge list (i < j to avoid duplicates)\n",
    "    src = np.repeat(np.arange(N), knn_idx.shape[1])\n",
    "    dst = knn_idx.reshape(-1)\n",
    "    edge_pairs = np.stack([src, dst], axis=1)\n",
    "    # Make undirected unique edges\n",
    "    edge_pairs = np.sort(edge_pairs, axis=1)\n",
    "    edge_pairs = np.unique(edge_pairs, axis=0)\n",
    "    g = ig.Graph(n=N, edges=edge_pairs.tolist(), directed=False)\n",
    "\n",
    "    weights = None\n",
    "    if weighted and knn_sim is not None:\n",
    "        # Map weights per edge; we take max of (i->j, j->i) if duplicates happened before unique\n",
    "        sim_map = {}\n",
    "        for i in range(N):\n",
    "            for k, j in enumerate(knn_idx[i]):\n",
    "                a, b = (i, int(j))\n",
    "                key = (min(a,b), max(a,b))\n",
    "                w = float(knn_sim[i, k])\n",
    "                sim_map[key] = max(sim_map.get(key, -1e9), w)\n",
    "        weights = [sim_map[(a,b)] for a,b in edge_pairs]\n",
    "\n",
    "    part = la.find_partition(g, la.RBConfigurationVertexPartition, weights=weights, resolution_parameter=resolution)\n",
    "    return np.array(part.membership), g, part\n",
    "\n",
    "labels_lei, g_lei, part_lei = leiden_from_knn(knn_idx.numpy(), knn_sim.numpy(), resolution=1.0, weighted=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b311b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def intrinsic_metrics(z, labels, max_points=20000):\n",
    "    # ignore noise label -1 for silhouette if present\n",
    "    z_np = z.numpy() if isinstance(z, torch.Tensor) else z\n",
    "    idx = np.arange(len(labels))\n",
    "    mask = labels != -1\n",
    "    use = idx[mask]\n",
    "    if len(use) < 2 or len(np.unique(labels[mask])) < 2:\n",
    "        return {'silhouette': np.nan, 'calinski_harabasz': np.nan, 'davies_bouldin': np.nan}\n",
    "    # downsample for speed\n",
    "    if len(use) > max_points:\n",
    "        use = np.random.RandomState(0).choice(use, size=max_points, replace=False)\n",
    "    s = silhouette_score(z_np[use], labels[use], metric='euclidean')\n",
    "    ch = calinski_harabasz_score(z_np[use], labels[use])\n",
    "    db = davies_bouldin_score(z_np[use], labels[use])\n",
    "    return {'silhouette': float(s), 'calinski_harabasz': float(ch), 'davies_bouldin': float(db)}\n",
    "\n",
    "m_in_hdb = intrinsic_metrics(z, labels_hdb)\n",
    "m_in_lei = intrinsic_metrics(z, labels_lei)\n",
    "print(\"HDBSCAN intrinsic:\", m_in_hdb)\n",
    "print(\"Leiden   intrinsic:\", m_in_lei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf637beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_purity_movie_director(labels, data, primary_ntype='movie'):\n",
    "    \"\"\"\n",
    "    For each movie, get its directors. For each cluster, take the majority director and compute purity.\n",
    "    Returns micro- and macro-averaged purity (exclude noise cluster -1).\n",
    "    \"\"\"\n",
    "    ei = data[('movie','to','director')].edge_index  # [2, E]\n",
    "    m, d = ei[0].cpu().numpy(), ei[1].cpu().numpy()\n",
    "    N = data[primary_ntype].num_nodes\n",
    "    labels = np.asarray(labels)\n",
    "    # build directors-per-movie lists\n",
    "    from collections import defaultdict, Counter\n",
    "    dirs_by_movie = defaultdict(list)\n",
    "    for mi, di in zip(m, d):\n",
    "        dirs_by_movie[int(mi)].append(int(di))\n",
    "    # per-cluster majority director\n",
    "    cluster_to_movies = {}\n",
    "    for mi in range(N):\n",
    "        c = int(labels[mi])\n",
    "        if c == -1:   # skip noise\n",
    "            continue\n",
    "        cluster_to_movies.setdefault(c, []).append(mi)\n",
    "    if not cluster_to_movies:\n",
    "        return {'micro_purity': np.nan, 'macro_purity': np.nan, 'n_clusters': 0}\n",
    "\n",
    "    purities = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for c, movies in cluster_to_movies.items():\n",
    "        # count directors across all movies in cluster\n",
    "        cnt = Counter()\n",
    "        for mi in movies:\n",
    "            cnt.update(dirs_by_movie.get(mi, []))\n",
    "        if len(cnt) == 0:\n",
    "            purities.append(0.0)\n",
    "            continue\n",
    "        maj_dir, maj_count = cnt.most_common(1)[0]\n",
    "        # total assignments = number of (movie, director) pairs in cluster\n",
    "        cluster_total = sum(cnt.values())\n",
    "        purities.append(maj_count / cluster_total)\n",
    "        total += cluster_total\n",
    "        correct += maj_count\n",
    "    micro = correct / total if total > 0 else np.nan\n",
    "    macro = float(np.mean(purities)) if purities else np.nan\n",
    "    return {'micro_purity': micro, 'macro_purity': macro, 'n_clusters': len(cluster_to_movies)}\n",
    "\n",
    "pur_hdb = cluster_purity_movie_director(labels_hdb, data, 'movie')\n",
    "pur_lei = cluster_purity_movie_director(labels_lei, data, 'movie')\n",
    "print(\"HDBSCAN movie→director purity:\", pur_hdb)\n",
    "print(\"Leiden   movie→director purity:\", pur_lei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6cf478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn (usually present)\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "def stability_ari_nmi(z, cluster_fn, n_runs=3):\n",
    "    parts = []\n",
    "    for s in range(n_runs):\n",
    "        np.random.seed(42 + s)\n",
    "        torch.manual_seed(42 + s)\n",
    "        labels, *_ = cluster_fn()\n",
    "        parts.append(np.asarray(labels))\n",
    "    # pairwise ARI/NMI\n",
    "    aris, nmis = [], []\n",
    "    for i in range(n_runs):\n",
    "        for j in range(i+1, n_runs):\n",
    "            aris.append(adjusted_rand_score(parts[i], parts[j]))\n",
    "            nmis.append(normalized_mutual_info_score(parts[i], parts[j]))\n",
    "    return {'ARI_mean': float(np.mean(aris)), 'NMI_mean': float(np.mean(nmis))}\n",
    "\n",
    "# Example: HDBSCAN stability over different min_samples\n",
    "stab_hdb = stability_ari_nmi(\n",
    "    z,\n",
    "    cluster_fn=lambda: cluster_hdbscan(z, min_cluster_size=20, min_samples=np.random.randint(5, 15)),\n",
    "    n_runs=3\n",
    ")\n",
    "print(\"HDBSCAN stability:\", stab_hdb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015551d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap-learn\n",
    "import umap\n",
    "def umap_embed(z, n_neighbors=30, min_dist=0.1, random_state=42):\n",
    "    Z = z.numpy() if isinstance(z, torch.Tensor) else z\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, random_state=random_state)\n",
    "    return reducer.fit_transform(Z)  # [N, 2]\n",
    "\n",
    "um = umap_embed(z, n_neighbors=30, min_dist=0.05)\n",
    "# Plot with your favorite tool; color by labels_hdb or labels_lei\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(um[:,0], um[:,1], c=labels_hdb, s=5, cmap='tab20', alpha=0.9)\n",
    "plt.title(\"UMAP of movie embeddings — HDBSCAN clusters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ccbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Export embeddings (leakage-safe) and normalize\n",
    "z = export_embeddings(model, splits['train_graph'], primary_ntype='movie', layers=2, batch_size=4096)\n",
    "z = torch.nn.functional.normalize(z, p=2, dim=1)\n",
    "\n",
    "# 2) k-NN graph\n",
    "knn_idx, knn_sim = knn_graph_cosine(z, k=30)\n",
    "\n",
    "# 3) Clustering\n",
    "labels_hdb, probs_hdb, _ = cluster_hdbscan(z, min_cluster_size=20, min_samples=10)\n",
    "labels_lei, g_lei, part_lei = leiden_from_knn(knn_idx.numpy(), knn_sim.numpy(), resolution=1.0, weighted=True)\n",
    "\n",
    "# 4) Metrics\n",
    "print(\"Intrinsic (HDBSCAN):\", intrinsic_metrics(z, labels_hdb))\n",
    "print(\"Intrinsic (Leiden):  \", intrinsic_metrics(z, labels_lei))\n",
    "print(\"Purity (movie→director, HDBSCAN):\", cluster_purity_movie_director(labels_hdb, data))\n",
    "print(\"Purity (movie→director, Leiden): \", cluster_purity_movie_director(labels_lei, data))\n",
    "\n",
    "# 5) Stability (optional)\n",
    "stab_hdb = stability_ari_nmi(z, lambda: cluster_hdbscan(z, 20, np.random.randint(5, 15)), n_runs=3)\n",
    "print(\"HDBSCAN stability:\", stab_hdb)\n",
    "\n",
    "# 6) UMAP (optional)\n",
    "um = umap_embed(z, n_neighbors=30, min_dist=0.05)\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(um[:,0], um[:,1], c=labels_hdb, s=5, cmap='tab20', alpha=0.9)\n",
    "plt.title(\"UMAP of movie embeddings — HDBSCAN clusters\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
